<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Minghao Chen</title>
  
  <meta name="author" content="Minghao Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Minghao Chen</name>
              </p>
              <p>Hi, I am currently a second-year DPhil student at <a href="https://www.robots.ox.ac.uk/~vgg/">Visual Geometry Group</a>, <a href="https://www.ox.ac.uk/">Oxford</a>, advised by <a href="https://www.robots.ox.ac.uk/~vedaldi/">Prof. Andrea Vedaldi</a> and <a href="https://campar.in.tum.de/Main/IroLaina">Dr. Iro Laina</a>.
               </p>
              <p> This summer, I am now doing a research scientist internship at Meta GenAI, London!
              </p>

              <p> Before that, I was a Ph.D. student at <a href="https://www.stonybrook.edu/">Stony Brook University</a>, supervised by <a href="https://www3.cs.stonybrook.edu/~hling/">Prof. Haibin Ling</a> from 2020 to 2022. I interned at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">MSR Asia</a> working with <a href="https://houwenpeng.com/">Dr. Houwen Peng </a> form 2020 to 2021.
                I received my M.S. from <a href="https://www.columbia.edu/">Columbia University</a> in 2020 and B.S. from <a href="https://ev.buaa.edu.cn/">Beihang University</a> in 2018.
              </p>

              <p style="color:red;">
                I am always open to new opportunities and collaborations. Feel free to contact me!

              </p>
              <p style="text-align:center">
                <a href="mailto:minghao@robots.ox.ac.uk">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=2JxJjCoAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/silent-chen">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/minghao.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/minghao_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>

        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, neural architecture search, generative model and 3D vision. My previous research is mainly about designing efficient and effective neural network automatically, while I am now focusing on generative models. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/DGE.gif" alt="b3do" width="200" height="150" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2404.18929">
                <papertitle>DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing</papertitle>
              </a>
              <br>
              <strong> Minghao Chen</strong>,
              <a href="https://scholar.google.de/citations?user=n9nXAPcAAAAJ&hl=en"> Iro Laina </a>,
              <a href="https://www.robots.ox.ac.uk/~vedaldi/"> Andrea Vedaldi </a>,

              <br>
              <em>ECCV </em>, 2024 &nbsp <font color="red"></font>
              <br>
              <a href="https://arxiv.org/abs/2404.18929">arXiv</a> /
              <a href="data/chen2024dge.bib">bibtex</a> /
              <a href="https://silent-chen.github.io/DGE/">project page</a> /
              <a href="https://github.com/silent-chen/DGE">code</a>
              <p>We introduce Direct Gaussian Editor (DGE), a novel method for fast 3D editing. We consider the task of 3D editing as a two-stage process, where the first stage focuses on achieving multi-view consistent 2D editing, followed by a secondary stage dedicated to precise 3D fitting.</p>
            </td>
          </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/shap_editor_local.gif" alt="b3do" width="200" height="150" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.09246">
                <papertitle>SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds</papertitle>
              </a>
              <br>
              <strong> Minghao Chen</strong>,
              <a href="https://scholar.google.com/citations?user=cDMqaTYAAAAJ&hl=en"> Junyu Xie </a>,
              <a href="https://scholar.google.de/citations?user=n9nXAPcAAAAJ&hl=en"> Iro Laina </a>,
              <a href="https://www.robots.ox.ac.uk/~vedaldi/"> Andrea Vedaldi </a>,

              <br>
              <em>CVPR </em>, 2024 &nbsp <font color="red"></font>
              <br>
              <a href="https://arxiv.org/abs/2312.09246">arXiv</a> /
              <a href="data/chen2023shapeditor.bib">bibtex</a> /
              <a href="https://silent-chen.github.io/Shap-Editor/">project page</a> /
              <a href="https://github.com/silent-chen/Shap-Editor">code</a> /
              <a href="https://huggingface.co/spaces/silentchen/Shap_Editor_demo">demo</a>
              <p>We present a method, named SHAP-EDITOR, aiming at fast 3D editing. We propose to learn a universal editing function that can be applied to different objects within one second.</p>
            </td>
          </tr>


        <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/layout-guidance.png" alt="b3do" width="200" height="150" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2304.03373">
                <papertitle>Training-Free Layout Control with Cross-Attention Guidance</papertitle>
              </a>
              <br>
              <strong> Minghao Chen</strong>,
              <a href="https://scholar.google.de/citations?user=n9nXAPcAAAAJ&hl=en"> Iro Laina </a>,
              <a href="https://www.robots.ox.ac.uk/~vedaldi/"> Andrea Vedaldi </a>,

              <br>
              <em>WACV</em>, 2024 &nbsp <font color="red"></font>
              <br>
              <a href="https://arxiv.org/abs/2304.03373">arXiv</a> /
              <a href="data/layoutguidance.bib">bibtex</a> /
              <a href="https://silent-chen.github.io/layout-guidance/">project page</a> /
              <a href="https://github.com/silent-chen/layout-guidance">code</a> /
              <a href="https://huggingface.co/spaces/silentchen/layout-guidance">demo</a>
              <p>We present a method for controlling the layout of images generated by large pre-trained text-to-image models by guiding the cross-attention patterns.</p>
            </td>
          </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/xclip_cake.gif" alt="b3do" width="200" height="150" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2208.02816.pdf">
                <papertitle>Expanding Language-Image Pretrained Models for General Video Recognition</papertitle>
              </a>
              <br>
              <a href="https://github.com/nbl97">Bolin Ni</a>,
              <a href="https://houwenpeng.com/"> Houwen Peng </a>,
              <strong> Minghao Chen</strong>,
              <a href="https://sy-zhang.github.io/"> Songyang Zhang</a>,
              <a href="https://people.ucas.ac.cn/~gfmeng"> Gaofeng Meng</a>,
              <a href="https://jianlong-fu.github.io/"> Jianlong Fu</a>,
              <a href="https://people.ucas.ac.cn/~xiangshiming"> Shiming Xiang</a>,
              <a href="https://www3.cs.stonybrook.edu/~hling/"> Haibin Ling</a>,
              <br>
              <em>ECCV</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2208.02816">arXiv</a> /
              <a href="data/xclip.bib">bibtex</a> /
              <a href="https://github.com/microsoft/VideoX/tree/master/X-CLIP">code</a>
              <p>A new framework adapting language-image foundation models to general video recognition.</p>
            </td>
          </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sss.png" alt="b3do" width="200" height="150" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://proceedings.neurips.cc/paper/2021/hash/48e95c45c8217961bf6cd7696d80d238-Abstract.html">
                <papertitle>Searching the Search Space of Vision Transformer</papertitle>
              </a>
              <br>
              <strong>Minghao Chen</strong>,
              <a href="https://github.com/wkcn"> Kan Wu</a>,
              <a href="https://github.com/nbl97"> Bolin Ni</a>,
              <a href="https://houwenpeng.com/"> Houwen Peng</a>,
              <a href="https://scholar.google.com/citations?user=7IZyaZsAAAAJ&hl=zh-CN"> Bei Liu</a>,
              <a href="https://jianlong-fu.github.io/"> Jianlong Fu</a>,
              <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en"> Hongyang Chao</a>,
              <a href="https://www3.cs.stonybrook.edu/~hling/"> Haibin Ling</a>,
              <br>
              <em>NeurIPS</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2111.14725">arXiv</a> /
              <a href="data/chen_neurips2021.bib">bibtex</a> /
              <a href="https://github.com/microsoft/Cream/tree/main/AutoFormerV2">code</a>
              <p>We propose to search the optimal search space of vision transformer models with AutoFormer training strategy.</p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/irpe.png" alt="b3do" width="200" height="150" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Wu_Rethinking_and_Improving_Relative_Position_Encoding_for_Vision_Transformer_ICCV_2021_paper.html">
                <papertitle>Rethinking and Improving Relative Position Encoding for Vision Transformer</papertitle>
              </a>
              <br>
              <a href="https://github.com/wkcn">Kan Wu</a>,
              <a href="https://houwenpeng.com/"> Houwen Peng</a>,
              <strong> Minghao Chen</strong>,
              <a href="https://jianlong-fu.github.io/"> Jianlong Fu</a>,
              <a href="https://scholar.google.com/citations?user=qnbpG6gAAAAJ&hl=en"> Hongyang Chao</a>,
              <br>
              <em>ICCV</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2107.14222">arXiv</a> /
              <a href="data/wu2021irpe.bib">bibtex</a> /
              <a href="https://github.com/microsoft/Cream/tree/main/iRPE">code</a>
              <p>A new relative position encoding methods dedicated to 2D images, considering directional relative distance modeling.</p>
            </td>
          </tr>


        <tr onmouseout="bs_stop()" onmouseover="bs_start()" bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="auto_one">
                <div class="auto_two" id='auto_image'><img src='images/autoformer_details.gif' width="200" height="150" ></div>
                <img src='images/autoformer.png' width="200" height="150", >
              </div>
              <script type="text/javascript">
                function bs_start() {
                  document.getElementById('auto_image').style.opacity = "1";
                }

                function bs_stop() {
                  document.getElementById('auto_image').style.opacity = "0";
                }
                bs_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Chen_AutoFormer_Searching_Transformers_for_Visual_Recognition_ICCV_2021_paper.html">
                <papertitle>AutoFormer: Searching Transformers for Visual Recognition</papertitle>
              </a>
              <br>
              <strong>Minghao Chen</strong>,
              <a href="https://houwenpeng.com/"> Houwen Peng </a>,
              <a href="https://jianlong-fu.github.io/"> Jianlong Fu</a>,
              <a href="https://www3.cs.stonybrook.edu/~hling/"> Haibin Ling</a>,
              <br>
              <em>ICCV</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2107.00651">arXiv</a> /
              <a href="data/chen2021autoformer.bib">bibtex</a> /
              <a href="https://github.com/microsoft/Cream/tree/main/AutoFormer">code</a> /
              <p></p>
              <p>A <font color="red"><strong>Once-for-all</strong></font> one-shot architecture search framework dedicated to vision transformer search.</p>
            </td>
          </tr>

      
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/21-neas.png" alt="b3do" width="200" height="150" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Chen_One-Shot_Neural_Ensemble_Architecture_Search_by_Diversity-Guided_Search_Space_Shrinking_CVPR_2021_paper.html">
                <papertitle>One-Shot Neural Ensemble Architecture Search by Diversity-Guided Search Space Shrinking</papertitle>
              </a>
              <br>
              <strong>Minghao Chen</strong>,
              <a href="https://houwenpeng.com/"> Houwen Peng</a>,
              <a href="https://jianlong-fu.github.io/"> Jianlong Fu</a>,
              <a href="https://www3.cs.stonybrook.edu/~hling/"> Haibin Ling</a>,
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2104.00597">arXiv</a> /
              <a href="data/Chen_2021_CVPR.bib">bibtex</a> /
              <a href="https://github.com/researchmm/NEAS">code</a>
              <p>We present a novel one-shot neural architecture method searching for optimal architectures for model ensemble.</p>
            </td>
          </tr>


        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Services</heading>
              <p><strong>Reviewer</strong></p>
              <p>CVPR 2022 2023 2024, ECCV 2022, ICCV 2023, NeurIPS 2023, 2024, ICLR 2024, WACV 2024, 2025, 3DV 2024, ACM MM 2022, 2021</p>
              <p><strong>Teaching Assistant</strong></p>
              <ul style="text-align:justify;height: 150px;">
              <li> COMS 4246 Algorithm for Data science, Columbia University, Department of Computer Science, Fall 2019</li>
              <li> COMS 4731 Computer Vision, Columbia University, Department of Computer Science, Fall 2019</li>
              </ul>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website template is borrowed from <a href="https://jonbarron.info/">Jon Barron</a>. Thanks!
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </tbody></table>
</body>

</html>
